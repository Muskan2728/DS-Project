# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CHShNGe1lXpAJNKTsKmr1Uv9bmH4bmm9
"""

# app.py
# Streamlit app for your MIT-BIH CNN models with SAME preprocessing logic
# ----------------------------------------------------------------------

import os
import numpy as np
import pandas as pd
import streamlit as st
import tensorflow as tf
from tensorflow.keras.models import load_model

import wfdb
import wfdb.processing
from sklearn.preprocessing import StandardScaler
import joblib

# -----------------------------
# CONFIG
# -----------------------------
MODELS_DIR = "models"          # folder with your .keras models
DEFAULT_FS = 360               # MIT-BIH sampling rate
WINDOW_SIZE = 180              # 90 left + 90 right (as in ipynb)
WIN_HALF_LEFT = WINDOW_SIZE // 2
WIN_HALF_RIGHT = WINDOW_SIZE // 2


# -----------------------------
# MODEL / ENCODER HELPERS
# -----------------------------
def list_available_models(models_dir: str):
    if not os.path.isdir(models_dir):
        return []
    models = [f for f in os.listdir(models_dir) if f.lower().endswith(".keras")]
    models.sort()
    return models


@st.cache_resource
def load_model_cached(model_filename: str):
    model_path = os.path.join(MODELS_DIR, model_filename)
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Model not found: {model_path}")
    model = load_model(model_path)
    return model


@st.cache_resource
def load_label_encoder():
    """Try to find a joblib LabelEncoder in models folder (optional)."""
    if not os.path.isdir(MODELS_DIR):
        return None

    for f in os.listdir(MODELS_DIR):
        if f.lower().endswith(".joblib"):
            try:
                enc = joblib.load(os.path.join(MODELS_DIR, f))
                return enc
            except Exception as e:
                st.warning(f"Found joblib file but could not load encoder: {e}")
                return None
    return None


# -----------------------------
# INPUT LOADING
# -----------------------------
def load_raw_signal_from_csv(uploaded_file) -> np.ndarray:
    """
    CSV options:
      - 1 numeric column  -> treat as one lead
      - 2 numeric columns -> sum them (like MLII + V5) same as notebook
    Returns: 1D numpy array: summed_signal_raw
    """
    df = pd.read_csv(uploaded_file)
    numeric = df.select_dtypes(include=[np.number])

    if numeric.shape[1] == 0:
        raise ValueError("CSV has no numeric columns.")

    if numeric.shape[1] == 1:
        summed_signal_raw = numeric.iloc[:, 0].values.astype("float32")
    else:
        # Take first two numeric columns and sum (same idea as mlii_raw + v5_raw)
        summed_signal_raw = (numeric.iloc[:, 0] + numeric.iloc[:, 1]).values.astype("float32")

    return summed_signal_raw


def load_raw_signal_from_npy(uploaded_file) -> np.ndarray:
    """
    NPY:
      - 1D array: already summed_signal_raw
      - 2D (N,2): sum columns 0 and 1 (MLII + V5)
    """
    arr = np.load(uploaded_file)
    arr = np.array(arr, dtype="float32")

    if arr.ndim == 1:
        return arr
    elif arr.ndim == 2:
        if arr.shape[1] == 1:
            return arr[:, 0]
        else:
            return (arr[:, 0] + arr[:, 1])
    else:
        raise ValueError(f"Unsupported npy shape {arr.shape}. Use 1D or 2D (N,1/2).")


# -----------------------------
# PREPROCESSING (same logic as IPYNB)
# -----------------------------
def preprocess_signal_to_beats(summed_signal_raw: np.ndarray,
                               fs: int = DEFAULT_FS,
                               window_size: int = WINDOW_SIZE):
    """
    This reproduces the logic from your notebook:
      - print basic stats
      - StandardScaler on summed_signal_raw.reshape(-1,1)
      - final_signal = scaled_summed_signal.flatten()
      - XQRS on summed_signal_raw (UNSCALED) to get QRS indices
      - for every R-peak make a 180-sample window [r-90, r+90]
      - discard windows going out of bounds
      - return beats as (N, 180, 1)
    """

    summed_signal_raw = np.asarray(summed_signal_raw, dtype="float32")
    n_samples = len(summed_signal_raw)

    if n_samples < window_size:
        raise ValueError(
            f"Signal too short ({n_samples} samples) for window size {window_size}."
        )

    # --- Same printing / scaling as your ipynb ---
    st.write(f"Original Mean: {np.mean(summed_signal_raw):.4f}")
    st.write(f"Original Std Dev: {np.std(summed_signal_raw):.4f}")

    summed_signal_reshaped = summed_signal_raw.reshape(-1, 1)

    scaler = StandardScaler()
    scaled_summed_signal = scaler.fit_transform(summed_signal_reshaped)

    st.write("After scaling (StandardScaler on summed signal):")
    st.write(f"Scaled Mean: {np.mean(scaled_summed_signal):.4f}")
    st.write(f"Scaled Std Dev: {np.std(scaled_summed_signal):.4f}")

    final_signal = scaled_summed_signal.flatten()

    # --- XQRS detection on raw summed signal (same as notebook) ---
    st.write("Running XQRS peak detection...")
    try:
        xqrs = wfdb.processing.XQRS(sig=summed_signal_raw, fs=fs)
        xqrs.detect()
        detected_peaks = xqrs.qrs_inds
        st.write(f"Found {len(detected_peaks)} R-peaks using XQRS.")
    except Exception as e:
        raise RuntimeError(f"XQRS detection failed: {e}")

    # --- Segment beats around each R-peak ---
    beats = []
    for r in detected_peaks:
        r_int = int(r)
        start = r_int - WIN_HALF_LEFT
        end = r_int + WIN_HALF_RIGHT

        if start >= 0 and end <= len(final_signal):
            segment = final_signal[start:end]
            if len(segment) == window_size:
                beats.append(segment)

    if len(beats) == 0:
        raise RuntimeError("No valid beats were segmented (0 windows kept).")

    X_beats = np.array(beats, dtype="float32").reshape(len(beats), window_size, 1)

    st.write(f"Total beats segmented: {X_beats.shape[0]}")
    st.write(f"X_beats shape: {X_beats.shape} (beats, length, channels)")

    return X_beats


# -----------------------------
# PREDICTION
# -----------------------------
def predict_beats(model, beats, label_encoder=None):
    probs = model.predict(beats, verbose=0)
    class_indices = np.argmax(probs, axis=1)

    if label_encoder is not None:
        try:
            class_labels = label_encoder.inverse_transform(class_indices)
        except Exception:
            class_labels = class_indices
    else:
        class_labels = class_indices

    return probs, class_indices, class_labels


# -----------------------------
# STREAMLIT UI
# -----------------------------
def main():
    st.title("MIT-BIH ECG CNN – Same Preprocessing as Training")

    st.write(
        """
        Yeh app tumhare wahi `.keras` models use karta hai jo notebooks me the
        (e.g. **my_summed_signal_model.keras**, **my_fine_tuned_noisy_model.keras**, etc.)
        aur **same preprocessing** apply karta hai:

        1. Agar CSV me 2 leads hain → unka **sum** (MLII + V5 type).
        2. `StandardScaler` se summed signal ko scale karta hai.
        3. XQRS se R-peaks detect karta hai.
        4. Har R-peak ke around 180 samples ka beat window banata hai.
        5. Beats ko `(N, 180, 1)` me reshape karke CNN pe `predict` karta hai.
        """
    )

    # --- Model selection ---
    models_list = list_available_models(MODELS_DIR)
    if not models_list:
        st.error(f"No .keras models found in '{MODELS_DIR}'.")
        return

    st.sidebar.header("Model")
    model_choice = st.sidebar.selectbox("Choose model (.keras):", models_list)
    model = load_model_cached(model_choice)

    label_encoder = load_label_encoder()
    if label_encoder is not None:
        st.sidebar.success("Label encoder loaded.")
    else:
        st.sidebar.info("No label encoder found – showing class indices only.")

    # --- FS selection (if needed) ---
    fs = st.sidebar.number_input("Sampling rate (Hz)", value=DEFAULT_FS, step=10)

    # --- Input file ---
    st.subheader("Upload raw ECG signal")
    st.write("File can be **CSV** (1–2 numeric columns) or **NPY** (1D or 2D with 1–2 leads).")

    upload_type = st.radio(
        "File type:",
        options=["CSV", "NPY"],
        horizontal=True,
    )

    if upload_type == "CSV":
        uploaded_file = st.file_uploader("Upload CSV", type=["csv"])
    else:
        uploaded_file = st.file_uploader("Upload NPY", type=["npy"])

    if uploaded_file is None:
        return

    # --- Load raw summed signal ---
    try:
        if upload_type == "CSV":
            summed_signal_raw = load_raw_signal_from_csv(uploaded_file)
        else:
            summed_signal_raw = load_raw_signal_from_npy(uploaded_file)
    except Exception as e:
        st.error(f"Error loading signal: {e}")
        return

    st.write(f"Loaded raw summed signal length: {len(summed_signal_raw)} samples")

    # Plot a small portion
    st.subheader("Raw signal preview (first 2000 samples)")
    n_preview = min(2000, len(summed_signal_raw))
    st.line_chart(summed_signal_raw[:n_preview])

    # --- Run preprocessing + prediction ---
    if st.button("Run preprocessing + prediction"):
        try:
            beats = preprocess_signal_to_beats(summed_signal_raw, fs=int(fs), window_size=WINDOW_SIZE)
        except Exception as e:
            st.error(f"Preprocessing failed: {e}")
            return

        with st.spinner("Running model.predict(...)"):
            probs, idx, labels = predict_beats(model, beats, label_encoder)

        st.subheader("Prediction summary")

        num_beats = beats.shape[0]
        if isinstance(labels, np.ndarray):
            labels = labels.tolist()

        df_res = pd.DataFrame(
            {
                "Beat #": np.arange(num_beats),
                "Predicted class": labels,
                "Predicted index": idx,
            }
        )
        st.dataframe(df_res, use_container_width=True)

        st.markdown("**Per-beat class probabilities (first 10 beats):**")
        max_show = min(10, num_beats)
        prob_df = pd.DataFrame(
            probs[:max_show],
            index=[f"Beat {i}" for i in range(max_show)],
        )
        st.dataframe(prob_df.style.format("{:.4f}"), use_container_width=True)

        # Show one example beat after preprocessing
        st.subheader("Example processed beat (first beat)")
        st.line_chart(beats[0, :, 0])


if __name__ == "__main__":
    main()